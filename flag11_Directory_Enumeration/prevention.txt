Why is it dangerous ?
In this example, we could read and found the flag that was inside the /.hidden folder. Meaning the attacker can read file that are not referenced in the website. We can imagine that in a real scenario, this folder could contain logs, configs files, README, sensitive keys,... which can lead to other more serious attacks. Which leads to another attack called Broken Access Control
In any case, a website will always have a robots.txt file. It is public. But providing sensitive path to real folders is a security breach.

How to prevent it ?
- Never expose configuration file or identification file in the webroot
- Deactivate repertory listing
- Don't put sensitive information on path shown in robots.txt file
- Control the access of the sensitive folders, with authentication for example.
- Minimise exposured files by encrypting paths
- Make sure that everyone involved in producing the website is fully aware of what information is considered sensitive. Sometimes seemingly harmless information can be much more useful to an attacker than people realize. Highlighting these dangers can help make sure that sensitive information is handled more securely in general by your organization.
-  Use generic error messages as much as possible. Don't provide attackers with clues about application behavior unnecessarily. 

Refs:
- https://owasp.org/www-community/attacks/Forced_browsing
- https://owasp.org/Top10/A01_2021-Broken_Access_Control/
- https://portswigger.net/web-security/information-disclosure